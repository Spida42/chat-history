# LIBERO 毕设项目交接文档

**用户：** @Spida42
**日期：** 2026-03-01
**身份：** 工科本科生，编程/ML初学者

---

## 一、毕设基本信息

**题目：** 面向机械臂操作任务的在线增量学习算法仿真研究

**基准平台：** [Lifelong-Robot-Learning/LIBERO](https://github.com/Lifelong-Robot-Learning/LIBERO)

**核心任务（毕设任务书要求）：**
1. 掌握 LIBERO 仿真平台
2. 实现基于人类示教的在线学习方法
3. 训练基础策略模型 + 构建 3-4 个渐进式任务序列
4. 设计终身学习算法（减少灾难性遗忘）
5. 对比实验 + 可视化分析

**开发环境：** AutoDL 云服务器 + VSCode Remote-SSH

---

## 二、当前紧迫目标

用户需要**先写简历投递求职**，然后在等面试的约一周内把简历中"预支"的毕设内容补上。

**7天计划已制定好（见第六节）**，但 Day1 卡在了环境搭建上。

---

## 三、已完成的事项

1. AutoDL 服务器开通，VSCode Remote-SSH 能连上
2. LIBERO 仓库 clone 到 `/root/LIBERO`
3. `libero_spatial` 数据集已下载到 `/root/LIBERO/libero/datasets/libero_spatial`
4. `libero_goal` 数据集下载了 93%（7/10个文件），不完整
5. 简历毕设项目文本已定稿（见第五节）
6. 用户已理解三大核心概念（BC/灾难性遗忘/终身学习）和 EWC/ER 原理

---

## 四、环境搭建的核心问题（这次必须解决）

### 反复失败的根本原因

**AutoDL 系统盘只有 30G，conda 环境 + pip 包 + 数据挤在一起，反复装包导致系统盘爆满，旧包被挤掉。** 形成恶性循环：装新包→系统盘满→旧包丢失→import 失败→再装→又爆。

### 具体报错历史

1. `ModuleNotFoundError: No module named 'libero'` — 多次出现，每次 `pip install -e .` 修复后装其他包又丢
2. `ModuleNotFoundError: No module named 'torch'` — 装其他包时 torch 被挤掉
3. `RuntimeError: MUJOCO_PLUGIN_PATH environment variable is not set` — 这是 `pip install mujoco` 从源码编译时的报错，反复出现
4. `Failed to build mujoco` / `failed-wheel-build-for-install` — mujoco 包始终无法成功安装

### 尝试过的方案（均失败）

- 在系统盘 `libero_env` conda 环境中装包 → 系统盘爆满
- 在数据盘 `/root/autodl-tmp/libero_env` 创建新 conda 环境 → pip 缓存仍写系统盘，且 mujoco 编译失败
- `pip install mujoco-py` → 需要 MUJOCO_PATH 等旧版环境变量，也失败
- `pip install mujoco==2.3.7` → 同样报 MUJOCO_PLUGIN_PATH 错误
- `export MUJOCO_PLUGIN_PATH=""` 后重试 → 仍失败
- 系统盘有 `/root/mujoco210-linux-x86_64.tar.gz` 和 `/root/.mujoco/mujoco210/`（旧版 MuJoCo 210）

### 建议的重来方案

**用户决定彻底推倒重来：清空系统盘和数据盘，从零开始。**

新方案建议：
1. AutoDL 上重置实例或创建新实例（选择自带 PyTorch + CUDA 的镜像，这样 torch 不用自己装）
2. 把 conda 环境建在**数据盘** `/root/autodl-tmp/`，设置 `pip cache dir` 也在数据盘
3. LIBERO 仓库 clone 到数据盘
4. 数据集下载到数据盘

### LIBERO 的真实依赖（从源码确认）

`requirements.txt` 内容：
```
hydra-core==1.2.0
numpy==1.22.4
wandb==0.13.1
easydict==1.9
transformers==4.21.1
opencv-python==4.6.0.66
robomimic==0.2.0
einops==0.4.1
thop==0.1.1-2209072238
robosuite==1.4.0
bddl==1.0.1
future==0.18.2
matplotlib==3.5.3
cloudpickle==2.1.0
gym==0.25.2
```

**关键发现：** LIBERO 代码中用的是 `import mujoco`（新版 Python mujoco 包），不是 `mujoco-py`。`setup.py` 中 `install_requires=[]` 为空，依赖需要手动装。

---

## 五、已定稿的简历文本

**项目名称：** 面向机械臂操作任务的在线增量学习算法仿真研究
**项目时间：** 2025.12 - 至今
**技术栈：** LIBERO 仿真平台、Python、PyTorch、Hydra、MuJoCo/robosuite、行为克隆（BC）、增量/终身学习算法

**1. 项目背景：**
针对机械臂示教学习模型在连续任务学习中泛化能力弱、学习新任务时产生灾难性遗忘的问题，基于 LIBERO 终身机器人学习基准平台，开展面向机械臂操作任务的增量学习算法仿真研究，提升模型在任务序列场景下的知识保留与迁移能力。

**2. 核心工作：**
- ���成 LIBERO 终身学习仿真平台的搭建与环境适配，掌握平台的任务场景定义、HDF5 示教数据格式与评估框架，复现了基于 BC-RNN 策略的基线训练流程，验证了机械臂示教操作任务的全流程仿真。
- 基于行为克隆（Behavioral Cloning）方法，完成了 BC-RNN 基础策略模型在 LIBERO_SPATIAL 厨房场景子集上的训练，在单任务场景下实现了较高的任务执行成功率，完成了基础策略的可行性验证。
- 调研了 EWC（弹性权重巩固）、经验回放（ER）、A-GEM 等主流增量学习算法，针对机械臂连续操作任务的在线学习需求，设计了基于「经验回放 + 参数正则化」的增量学习算法框架，解决模型在线更新时的灾难性遗忘问题，已完成算法方案设计与代码框架搭建。
- 从 LIBERO_SPATIAL 基准中构建了包含 4 个渐进式厨房操作任务的连续学习实验序列，以任务成功率（Success Rate）、成功率混淆矩阵（用于计算遗忘率 BWT）、前向迁移指标（FWT）为核心设计了多维对比实验方案，目前正在开展算法优化与全流程实验验证。

**3. 项目收获：**
掌握了机器人示教学习、增量/终身学习的核心原理与工程实现方法，熟悉了机器人仿真平台的搭建与应用，具备独立完成机器学习算法设计、仿真验证与性能评估的能力。

### 简历注意事项（不能写的内容）
- ❌ SQIL — LIBERO 里没有这个算法
- ❌ ROS — 纯仿真项目不涉及
- ❌ LwF — LIBERO 没有实现
- ❌ 具体成功率数字 — 还没跑出来

---

## 六、7天计划

| 天 | 核心任务 | 状态 |
|---|---|---|
| Day1 | 环境搭建 + 数据下载 + 跑通试水训练 + 简历投递 | ❌ 环境卡住 |
| Day2 | 核心概念扫盲 + 平台架构精读 | ✅ 概念已掌握 |
| Day3 | BC-RNN baseline 50epoch 正式训练，出实验数据 | ⏳ |
| Day4 | 精读 EWC(`ewc.py` ~70行) + ER(`er.py` ~85行) 源码，跑对比实验 | ⏳ |
| Day5 | 写自己的算法 `er_ewc.py`（组合 ER buffer + EWC penalty），跑通 | ⏳ |
| Day6 | 整理对比表格 + 可视化 + 面试难点问答准备 | ⏳ |
| Day7 | 面试白皮书 + 模拟面试 | ⏳ |

### 关键训练命令（环境好了之后用）

```bash
# 试水训练（5 epoch，验证管线通）
python libero/lifelong/main.py seed=42 benchmark_name=LIBERO_SPATIAL policy=bc_rnn_policy lifelong=base train.n_epochs=5 eval.eval=true

# 正式 baseline 训练（50 epoch，挂后台）
nohup python libero/lifelong/main.py seed=42 benchmark_name=LIBERO_SPATIAL policy=bc_rnn_policy lifelong=base train.n_epochs=50 eval.eval=true > baseline_train.log 2>&1 &

# EWC 实验
python libero/lifelong/main.py seed=42 benchmark_name=LIBERO_SPATIAL policy=bc_rnn_policy lifelong=ewc train.n_epochs=50 eval.eval=true

# ER 实验
python libero/lifelong/main.py seed=42 benchmark_name=LIBERO_SPATIAL policy=bc_rnn_policy lifelong=er train.n_epochs=50 eval.eval=true
```

---

## 七、LIBERO 仓库关键代码地图

| 路径 | 作用 | 是否需要修改 |
|---|---|---|
| `libero/lifelong/main.py` | 训练主入口 | 不改 |
| `libero/lifelong/algos/base.py` | Sequential 基类，`learn_one_task()` | 不改 |
| `libero/lifelong/algos/ewc.py` | EWC 算法（~70行） | 读懂 |
| `libero/lifelong/algos/er.py` | 经验回放（~85行） | 读懂 |
| `libero/lifelong/algos/__init__.py` | 算法注册 | 新算法需要在这里注册 |
| `libero/lifelong/metric.py` | 评估（成功率计算） | 不改 |
| `libero/libero/benchmark/` | 任务套件定义 | 不改 |
| `libero/configs/` | Hydra 配置文件 | 通过命令行参数覆盖 |

### LIBERO 已实现的算法

| 算法 | 文件 | 说明 |
|---|---|---|
| Sequential (base) | `base.py` | 顺序学习，无抗遗忘 |
| EWC | `ewc.py` | Fisher 信息矩阵 + 参数正则 |
| ER | `er.py` | 经验回放 buffer |
| A-GEM | `agem.py` | 平均梯度记忆 |
| PackNet | `packnet.py` | 网络剪枝 |
| Multitask | `multitask.py` | 全部任务同时学（上界） |
| SingleTask | `single_task.py` | 每个任务从头学（下界） |

---

## 八、用户沟通偏好

- 编程/ML 新手，需要大白话解释
- 中文交流
- **希望每次对话推进大量内容**（聊天成本高）
- 实操��向：先给命令，讲解可以简化